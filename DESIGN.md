# System Design – VirusTotal + Gemini AI File Scanner

## 1. Overview

This project is a cloud-deployed Golang web application that allows users to upload files, scan them using VirusTotal, and receive a structured explanation generated by Google Gemini AI.

The primary challenge was safely handling large file uploads while coordinating asynchronous external API calls and AI summarization.

The priorities were:

* Secure file handling
* Efficient API orchestration
* Clear separation of responsibilities
* Production-aware logging
* Cloud-ready deployment
* Proper secret management

---

## 2. High-Level Architecture

The system is organized into three primary components:

### Client Layer

* Runs in the browser
* Handles file upload via multipart form submission
* Sends asynchronous HTTP requests to the backend
* Renders structured detection statistics
* Displays AI-generated explanations
* Maintains session-level scan history

### Backend Layer

* Implemented in Golang using `net/http`
* Serves as the security boundary for file uploads
* Orchestrates VirusTotal and Gemini API calls
* Performs deterministic risk classification
* Handles structured error responses
* Implements request-level logging and timeout control

The backend is organized into:

* **Handler layer** – HTTP routing and request orchestration
* **Service layer** – VirusTotal and Gemini integrations
* **Utility layer** – hashing, validation, and logging

This modular structure keeps responsibilities clearly separated and reduces cross-dependencies.

### External Services

* **VirusTotal API**

  * Hash lookup
  * File upload
  * Analysis polling

* **Gemini API**

  * Converts detection statistics into structured explanations

---

## 3. File Analysis Workflow

This section describes how an uploaded file is validated, analyzed, and processed through VirusTotal.

### 3.1 Upload Validation & Constraints

VirusTotal supports two upload modes:

* File size ≤ 32MB -> Direct upload
* File size > 32MB -> Upload via special upload URL
* File size > 650MB -> Rejected

The backend enforces:

* Hard 650MB cap using `http.MaxBytesReader`
* Multipart parsing limits
* Temporary disk storage using `os.CreateTemp`
* SHA-256 hashing before any upload

Temporary files are deleted after processing.

Uploaded files are never executed or persisted beyond request scope.

---

### 3.2 Hash-First Lookup Strategy

Before uploading any file:

1. Compute SHA-256
2. Query `/files/{hash}` on VirusTotal

If an existing report is found, it is returned immediately.

This avoids:

* Redundant uploads
* Wasted API quota
* Additional latency

---

### 3.3 Upload & Analysis Flow

If the hash is not found:

* File Size ≤ 32MB → `POST /files`
* File Size > 32MB →

  * `GET /files/upload_url`
  * Upload file to returned URL

After upload:

* Poll `/analyses/{id}`
* Stop when `status = completed`
* Extract detection statistics:

  * malicious
  * suspicious
  * harmless
  * undetected

Polling is bounded and timeout-protected.

---

### 3.4 Handling Long-Running Operations

Large files may require:

* Extended upload time
* VirusTotal processing time

To control execution:

* `context.WithTimeout` (10 minutes)
* Fixed polling interval
* Maximum polling attempts
* Graceful timeout handling

This prevents:

* Infinite polling
* Hanging goroutines
* Resource exhaustion

The current implementation is synchronous, meaning the HTTP request blocks until analysis completes.

Trade-off:

* Simpler architecture
* Longer response time for large files

---

## 4. AI Integration (Gemini)

Gemini is used only to transform detection numbers into a structured explanation.

Workflow:

1. Extract detection statistics
2. Derive deterministic risk level
3. Send structured prompt to Gemini

### Risk Classification

* malicious > 0 -> High Risk
* suspicious > 0 -> Medium Risk
* Otherwise -> Low Risk

AI does not determine risk. It only explains it.

Prompt constraints:

* No speculation
* No behavioral assumptions
* Strict output formatting
* Clear structured sections

This avoids hallucination and keeps the system deterministic.

---

## 5. Error Handling Strategy

All external API responses are explicitly handled with structured branching logic.

### VirusTotal

* 200 → success
* 401 → invalid API key
* 404 → hash not found
* 429 → rate limit
* Other codes → structured error

### Gemini

* 429 -> quota exceeded
* Non-200 -> formatted API error
* Empty responses -> safe fallback

Errors return:

* Proper HTTP status
* Structured JSON
* No stack traces
* No secret leakage

The system never trusts external APIs blindly.

---

## 6. Logging & Observability

Structured logging is implemented using `zerolog`, enabling contextual, structured logs with request identifiers and duration metrics.

Logged events include:

* Scan started
* File received
* Hash computed
* VirusTotal lookup
* Upload
* Polling progress
* AI generation
* Completion (with duration)

This makes it possible to:

* Debug large uploads
* Diagnose API failures
* Measure performance
* Trace user flows

Logs are written to `server.log`.

---

## 7. Security & Deployment Architecture

### Secret Management

Secrets are handled differently in development and production.

**Local development**

* API keys are stored in a `.env` file for convenience.
* `.env` is excluded from Git via `.gitignore`.
* No credentials are committed to the repository.

**Production (EC2)**

* Secrets are stored in AWS Secrets Manager.
* The EC2 instance uses an IAM role to retrieve secrets securely.
* No static AWS credentials are stored in code or environment files.
* Docker images do not contain embedded secrets.

This separation ensures that development remains simple while production follows secure secret management practices.

---

### File Handling Security

* Uploaded files are written to temporary disk storage only.
* Files are deleted immediately after processing.
* Uploaded files are never executed.
* Strict size limits are enforced at the HTTP layer.
* No raw file contents are exposed to clients.

This reduces the risk of persistent malicious file storage and resource exhaustion.

---

### Deployment Architecture

The system is deployed using:

* AWS EC2 (Ubuntu)
* Docker (multi-stage build)
* GitHub Actions for CI/CD
* AWS Secrets Manager
* IAM role-based access control

Docker uses:

* Multi-stage build
* Go binary compiled in builder stage
* Minimal Alpine runtime image
* Static assets copied into container
* Port 8080 exposed internally

This reduces image size and attack surface.

---

### CI/CD Deployment Flow

Deployment is automated via GitHub Actions:

1. Push to `main`
2. GitHub Actions triggers workflow
3. SSH into EC2
4. Pull latest code
5. Rebuild Docker container
6. Restart service

This ensures repeatable and consistent deployments.

---

## 8. Future Improvements

The current implementation prioritizes simplicity. The following improvements would meaningfully strengthen scalability and security.

### 1. Persistent Scan Storage

Currently, scan history is session-based and lost when the container restarts.

Improvement:

* Integrate a database (e.g., PostgreSQL or DynamoDB)
* Persist scan metadata and results
* Decouple application state from container lifecycle

This would enable historical lookup and improve reliability.

---

### 2. Asynchronous Processing Model

The current design blocks the HTTP request until analysis completes.

Improvement:

* Move scan execution to a background worker
* Return a job ID immediately
* Provide a status endpoint
* Optionally stream updates via SSE or WebSocket

This would improve scalability and reduce long request durations.

---

### 3. Secure Deployment Without Public SSH

Deployment currently relies on SSH, which requires port 22 exposure.

Improvement:

* Replace SSH-based deployment with AWS Systems Manager (SSM)
* Eliminate public SSH access
* Use IAM-based remote command execution

This would reduce attack surface significantly.

---

### 4. File-Type & Abuse Controls

Currently, only size limits are enforced.

Improvement:

* Enforce MIME-type validation
* Restrict unsupported or high-risk file types
* Add per-IP rate limiting

This would reduce misuse and improve security posture.

---

### 5. HTTPS & Reverse Proxy

The application currently runs over HTTP.

Improvement:

* Add reverse proxy (Nginx or ALB)
* Enable HTTPS via AWS ACM
* Enforce secure headers

This is essential for production-grade deployment.

---

## Conclusion 

This system reflects several deliberate engineering decisions made during implementation:

- Risk classification is deterministic and rule-based, ensuring that security decisions are explainable and not AI-dependent.
- SHA-256 hash lookup occurs before upload to minimize redundant submissions, reduce latency, and conserve API quota.
- Upload limits are enforced at the HTTP layer to protect server resources.
- Long-running operations are bounded using timeouts and controlled polling.
- External APIs are handled explicitly with structured error responses.
- Secrets are separated from the codebase and managed securely in production.
- Request-level logging enables traceability and operational debugging.

Together, these decisions result in a system that:

- Safely handles large file uploads
- Orchestrates external security APIs in a controlled manner
- Uses AI for summarization without delegating security logic
- Maintains clear architectural separation
- Supports automated cloud deployment

